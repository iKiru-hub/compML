{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "X, y = load_breast_cancer(return_X_y=True, as_frame=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "       mean radius  mean texture  mean perimeter    mean area  \\\ncount   569.000000    569.000000      569.000000   569.000000   \nmean     14.127292     19.289649       91.969033   654.889104   \nstd       3.524049      4.301036       24.298981   351.914129   \nmin       6.981000      9.710000       43.790000   143.500000   \n25%      11.700000     16.170000       75.170000   420.300000   \n50%      13.370000     18.840000       86.240000   551.100000   \n75%      15.780000     21.800000      104.100000   782.700000   \nmax      28.110000     39.280000      188.500000  2501.000000   \n\n       mean smoothness  mean compactness  mean concavity  mean concave points  \\\ncount       569.000000        569.000000      569.000000           569.000000   \nmean          0.096360          0.104341        0.088799             0.048919   \nstd           0.014064          0.052813        0.079720             0.038803   \nmin           0.052630          0.019380        0.000000             0.000000   \n25%           0.086370          0.064920        0.029560             0.020310   \n50%           0.095870          0.092630        0.061540             0.033500   \n75%           0.105300          0.130400        0.130700             0.074000   \nmax           0.163400          0.345400        0.426800             0.201200   \n\n       mean symmetry  mean fractal dimension  ...  worst radius  \\\ncount     569.000000              569.000000  ...    569.000000   \nmean        0.181162                0.062798  ...     16.269190   \nstd         0.027414                0.007060  ...      4.833242   \nmin         0.106000                0.049960  ...      7.930000   \n25%         0.161900                0.057700  ...     13.010000   \n50%         0.179200                0.061540  ...     14.970000   \n75%         0.195700                0.066120  ...     18.790000   \nmax         0.304000                0.097440  ...     36.040000   \n\n       worst texture  worst perimeter   worst area  worst smoothness  \\\ncount     569.000000       569.000000   569.000000        569.000000   \nmean       25.677223       107.261213   880.583128          0.132369   \nstd         6.146258        33.602542   569.356993          0.022832   \nmin        12.020000        50.410000   185.200000          0.071170   \n25%        21.080000        84.110000   515.300000          0.116600   \n50%        25.410000        97.660000   686.500000          0.131300   \n75%        29.720000       125.400000  1084.000000          0.146000   \nmax        49.540000       251.200000  4254.000000          0.222600   \n\n       worst compactness  worst concavity  worst concave points  \\\ncount         569.000000       569.000000            569.000000   \nmean            0.254265         0.272188              0.114606   \nstd             0.157336         0.208624              0.065732   \nmin             0.027290         0.000000              0.000000   \n25%             0.147200         0.114500              0.064930   \n50%             0.211900         0.226700              0.099930   \n75%             0.339100         0.382900              0.161400   \nmax             1.058000         1.252000              0.291000   \n\n       worst symmetry  worst fractal dimension  \ncount      569.000000               569.000000  \nmean         0.290076                 0.083946  \nstd          0.061867                 0.018061  \nmin          0.156500                 0.055040  \n25%          0.250400                 0.071460  \n50%          0.282200                 0.080040  \n75%          0.317900                 0.092080  \nmax          0.663800                 0.207500  \n\n[8 rows x 30 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean radius</th>\n      <th>mean texture</th>\n      <th>mean perimeter</th>\n      <th>mean area</th>\n      <th>mean smoothness</th>\n      <th>mean compactness</th>\n      <th>mean concavity</th>\n      <th>mean concave points</th>\n      <th>mean symmetry</th>\n      <th>mean fractal dimension</th>\n      <th>...</th>\n      <th>worst radius</th>\n      <th>worst texture</th>\n      <th>worst perimeter</th>\n      <th>worst area</th>\n      <th>worst smoothness</th>\n      <th>worst compactness</th>\n      <th>worst concavity</th>\n      <th>worst concave points</th>\n      <th>worst symmetry</th>\n      <th>worst fractal dimension</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>...</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>14.127292</td>\n      <td>19.289649</td>\n      <td>91.969033</td>\n      <td>654.889104</td>\n      <td>0.096360</td>\n      <td>0.104341</td>\n      <td>0.088799</td>\n      <td>0.048919</td>\n      <td>0.181162</td>\n      <td>0.062798</td>\n      <td>...</td>\n      <td>16.269190</td>\n      <td>25.677223</td>\n      <td>107.261213</td>\n      <td>880.583128</td>\n      <td>0.132369</td>\n      <td>0.254265</td>\n      <td>0.272188</td>\n      <td>0.114606</td>\n      <td>0.290076</td>\n      <td>0.083946</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>3.524049</td>\n      <td>4.301036</td>\n      <td>24.298981</td>\n      <td>351.914129</td>\n      <td>0.014064</td>\n      <td>0.052813</td>\n      <td>0.079720</td>\n      <td>0.038803</td>\n      <td>0.027414</td>\n      <td>0.007060</td>\n      <td>...</td>\n      <td>4.833242</td>\n      <td>6.146258</td>\n      <td>33.602542</td>\n      <td>569.356993</td>\n      <td>0.022832</td>\n      <td>0.157336</td>\n      <td>0.208624</td>\n      <td>0.065732</td>\n      <td>0.061867</td>\n      <td>0.018061</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>6.981000</td>\n      <td>9.710000</td>\n      <td>43.790000</td>\n      <td>143.500000</td>\n      <td>0.052630</td>\n      <td>0.019380</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.106000</td>\n      <td>0.049960</td>\n      <td>...</td>\n      <td>7.930000</td>\n      <td>12.020000</td>\n      <td>50.410000</td>\n      <td>185.200000</td>\n      <td>0.071170</td>\n      <td>0.027290</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.156500</td>\n      <td>0.055040</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>11.700000</td>\n      <td>16.170000</td>\n      <td>75.170000</td>\n      <td>420.300000</td>\n      <td>0.086370</td>\n      <td>0.064920</td>\n      <td>0.029560</td>\n      <td>0.020310</td>\n      <td>0.161900</td>\n      <td>0.057700</td>\n      <td>...</td>\n      <td>13.010000</td>\n      <td>21.080000</td>\n      <td>84.110000</td>\n      <td>515.300000</td>\n      <td>0.116600</td>\n      <td>0.147200</td>\n      <td>0.114500</td>\n      <td>0.064930</td>\n      <td>0.250400</td>\n      <td>0.071460</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>13.370000</td>\n      <td>18.840000</td>\n      <td>86.240000</td>\n      <td>551.100000</td>\n      <td>0.095870</td>\n      <td>0.092630</td>\n      <td>0.061540</td>\n      <td>0.033500</td>\n      <td>0.179200</td>\n      <td>0.061540</td>\n      <td>...</td>\n      <td>14.970000</td>\n      <td>25.410000</td>\n      <td>97.660000</td>\n      <td>686.500000</td>\n      <td>0.131300</td>\n      <td>0.211900</td>\n      <td>0.226700</td>\n      <td>0.099930</td>\n      <td>0.282200</td>\n      <td>0.080040</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>15.780000</td>\n      <td>21.800000</td>\n      <td>104.100000</td>\n      <td>782.700000</td>\n      <td>0.105300</td>\n      <td>0.130400</td>\n      <td>0.130700</td>\n      <td>0.074000</td>\n      <td>0.195700</td>\n      <td>0.066120</td>\n      <td>...</td>\n      <td>18.790000</td>\n      <td>29.720000</td>\n      <td>125.400000</td>\n      <td>1084.000000</td>\n      <td>0.146000</td>\n      <td>0.339100</td>\n      <td>0.382900</td>\n      <td>0.161400</td>\n      <td>0.317900</td>\n      <td>0.092080</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>28.110000</td>\n      <td>39.280000</td>\n      <td>188.500000</td>\n      <td>2501.000000</td>\n      <td>0.163400</td>\n      <td>0.345400</td>\n      <td>0.426800</td>\n      <td>0.201200</td>\n      <td>0.304000</td>\n      <td>0.097440</td>\n      <td>...</td>\n      <td>36.040000</td>\n      <td>49.540000</td>\n      <td>251.200000</td>\n      <td>4254.000000</td>\n      <td>0.222600</td>\n      <td>1.058000</td>\n      <td>1.252000</td>\n      <td>0.291000</td>\n      <td>0.663800</td>\n      <td>0.207500</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 30 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 30 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   mean radius              569 non-null    float64\n",
      " 1   mean texture             569 non-null    float64\n",
      " 2   mean perimeter           569 non-null    float64\n",
      " 3   mean area                569 non-null    float64\n",
      " 4   mean smoothness          569 non-null    float64\n",
      " 5   mean compactness         569 non-null    float64\n",
      " 6   mean concavity           569 non-null    float64\n",
      " 7   mean concave points      569 non-null    float64\n",
      " 8   mean symmetry            569 non-null    float64\n",
      " 9   mean fractal dimension   569 non-null    float64\n",
      " 10  radius error             569 non-null    float64\n",
      " 11  texture error            569 non-null    float64\n",
      " 12  perimeter error          569 non-null    float64\n",
      " 13  area error               569 non-null    float64\n",
      " 14  smoothness error         569 non-null    float64\n",
      " 15  compactness error        569 non-null    float64\n",
      " 16  concavity error          569 non-null    float64\n",
      " 17  concave points error     569 non-null    float64\n",
      " 18  symmetry error           569 non-null    float64\n",
      " 19  fractal dimension error  569 non-null    float64\n",
      " 20  worst radius             569 non-null    float64\n",
      " 21  worst texture            569 non-null    float64\n",
      " 22  worst perimeter          569 non-null    float64\n",
      " 23  worst area               569 non-null    float64\n",
      " 24  worst smoothness         569 non-null    float64\n",
      " 25  worst compactness        569 non-null    float64\n",
      " 26  worst concavity          569 non-null    float64\n",
      " 27  worst concave points     569 non-null    float64\n",
      " 28  worst symmetry           569 non-null    float64\n",
      " 29  worst fractal dimension  569 non-null    float64\n",
      "dtypes: float64(30)\n",
      "memory usage: 133.5 KB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "import tensorflow as tf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.66)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "X_train = tf.constant(X_train, dtype=tf.float32)\n",
    "X_test = tf.constant(X_test, dtype=tf.float32)\n",
    "y_train = tf.constant(y_train, dtype=tf.float32)\n",
    "y_test = tf.constant(y_test, dtype=tf.float32)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def logistic_function(x, betas):\n",
    "    #assert len(x) == len(betas) + 1\n",
    "    feature_input = x*betas[1:]\n",
    "    exp_term = -(betas[0] + tf.reduce_sum(feature_input, axis=1))\n",
    "    return 1 / (1+tf.math.exp(exp_term))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def cross_entropy(y_true, prediction):\n",
    "    ce_eps = 1e-7\n",
    "    prediction = tf.clip_by_value(prediction, ce_eps, 1-ce_eps) #clip to avoid numerical problems with log(0). In general, logit= 0 or 1 shouldn't be possible, but these values come up due to the limited float32 precision\n",
    "    return -tf.reduce_mean(y_true * tf.math.log(prediction) + (1-y_true)*tf.math.log(1-prediction))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Custom regression + tensorflow optimizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.6428133, shape=(), dtype=float32)\n",
      "tf.Tensor(0.5145679, shape=(), dtype=float32)\n",
      "tf.Tensor(0.44082108, shape=(), dtype=float32)\n",
      "tf.Tensor(0.3916763, shape=(), dtype=float32)\n",
      "tf.Tensor(0.355891, shape=(), dtype=float32)\n",
      "tf.Tensor(0.32835555, shape=(), dtype=float32)\n",
      "tf.Tensor(0.30635336, shape=(), dtype=float32)\n",
      "tf.Tensor(0.28828293, shape=(), dtype=float32)\n",
      "tf.Tensor(0.27312836, shape=(), dtype=float32)\n",
      "tf.Tensor(0.26020867, shape=(), dtype=float32)\n",
      "tf.Tensor(0.24904707, shape=(), dtype=float32)\n",
      "tf.Tensor(0.23929806, shape=(), dtype=float32)\n",
      "tf.Tensor(0.23070396, shape=(), dtype=float32)\n",
      "tf.Tensor(0.22306801, shape=(), dtype=float32)\n",
      "tf.Tensor(0.2162369, shape=(), dtype=float32)\n",
      "tf.Tensor(0.21008909, shape=(), dtype=float32)\n",
      "tf.Tensor(0.20452678, shape=(), dtype=float32)\n",
      "tf.Tensor(0.19947031, shape=(), dtype=float32)\n",
      "tf.Tensor(0.194854, shape=(), dtype=float32)\n",
      "tf.Tensor(0.19062322, shape=(), dtype=float32)\n",
      "tf.Tensor(0.18673193, shape=(), dtype=float32)\n",
      "tf.Tensor(0.18314137, shape=(), dtype=float32)\n",
      "tf.Tensor(0.17981824, shape=(), dtype=float32)\n",
      "tf.Tensor(0.17673421, shape=(), dtype=float32)\n",
      "tf.Tensor(0.17386468, shape=(), dtype=float32)\n",
      "tf.Tensor(0.17118832, shape=(), dtype=float32)\n",
      "tf.Tensor(0.16868657, shape=(), dtype=float32)\n",
      "tf.Tensor(0.16634311, shape=(), dtype=float32)\n",
      "tf.Tensor(0.16414362, shape=(), dtype=float32)\n",
      "tf.Tensor(0.16207547, shape=(), dtype=float32)\n",
      "tf.Tensor(0.16012742, shape=(), dtype=float32)\n",
      "tf.Tensor(0.15828946, shape=(), dtype=float32)\n",
      "tf.Tensor(0.15655276, shape=(), dtype=float32)\n",
      "tf.Tensor(0.15490928, shape=(), dtype=float32)\n",
      "tf.Tensor(0.15335184, shape=(), dtype=float32)\n",
      "tf.Tensor(0.15187405, shape=(), dtype=float32)\n",
      "tf.Tensor(0.1504701, shape=(), dtype=float32)\n",
      "tf.Tensor(0.14913464, shape=(), dtype=float32)\n",
      "tf.Tensor(0.14786299, shape=(), dtype=float32)\n",
      "tf.Tensor(0.1466507, shape=(), dtype=float32)\n",
      "tf.Tensor(0.14549391, shape=(), dtype=float32)\n",
      "tf.Tensor(0.14438894, shape=(), dtype=float32)\n",
      "tf.Tensor(0.14333251, shape=(), dtype=float32)\n",
      "tf.Tensor(0.14232157, shape=(), dtype=float32)\n",
      "tf.Tensor(0.14135334, shape=(), dtype=float32)\n",
      "tf.Tensor(0.14042526, shape=(), dtype=float32)\n",
      "tf.Tensor(0.13953497, shape=(), dtype=float32)\n",
      "tf.Tensor(0.13868026, shape=(), dtype=float32)\n",
      "tf.Tensor(0.13785914, shape=(), dtype=float32)\n",
      "tf.Tensor(0.13706973, shape=(), dtype=float32)\n",
      "tf.Tensor(0.13631031, shape=(), dtype=float32)\n",
      "tf.Tensor(0.13557926, shape=(), dtype=float32)\n",
      "tf.Tensor(0.13487506, shape=(), dtype=float32)\n",
      "tf.Tensor(0.13419639, shape=(), dtype=float32)\n",
      "tf.Tensor(0.13354188, shape=(), dtype=float32)\n",
      "tf.Tensor(0.13291033, shape=(), dtype=float32)\n",
      "tf.Tensor(0.13230063, shape=(), dtype=float32)\n",
      "tf.Tensor(0.13171172, shape=(), dtype=float32)\n",
      "tf.Tensor(0.13114257, shape=(), dtype=float32)\n",
      "tf.Tensor(0.13059227, shape=(), dtype=float32)\n",
      "tf.Tensor(0.13005994, shape=(), dtype=float32)\n",
      "tf.Tensor(0.12954478, shape=(), dtype=float32)\n",
      "tf.Tensor(0.12904601, shape=(), dtype=float32)\n",
      "tf.Tensor(0.12856288, shape=(), dtype=float32)\n",
      "tf.Tensor(0.12809473, shape=(), dtype=float32)\n",
      "tf.Tensor(0.12764087, shape=(), dtype=float32)\n",
      "tf.Tensor(0.12720075, shape=(), dtype=float32)\n",
      "tf.Tensor(0.12677377, shape=(), dtype=float32)\n",
      "tf.Tensor(0.12635936, shape=(), dtype=float32)\n",
      "tf.Tensor(0.12595704, shape=(), dtype=float32)\n",
      "tf.Tensor(0.12556632, shape=(), dtype=float32)\n",
      "tf.Tensor(0.12518671, shape=(), dtype=float32)\n",
      "tf.Tensor(0.124817766, shape=(), dtype=float32)\n",
      "tf.Tensor(0.12445914, shape=(), dtype=float32)\n",
      "tf.Tensor(0.12411036, shape=(), dtype=float32)\n",
      "tf.Tensor(0.12377109, shape=(), dtype=float32)\n",
      "tf.Tensor(0.12344093, shape=(), dtype=float32)\n",
      "tf.Tensor(0.12311961, shape=(), dtype=float32)\n",
      "tf.Tensor(0.122806765, shape=(), dtype=float32)\n",
      "tf.Tensor(0.12250208, shape=(), dtype=float32)\n",
      "tf.Tensor(0.12220528, shape=(), dtype=float32)\n",
      "tf.Tensor(0.12191608, shape=(), dtype=float32)\n",
      "tf.Tensor(0.121634215, shape=(), dtype=float32)\n",
      "tf.Tensor(0.12135942, shape=(), dtype=float32)\n",
      "tf.Tensor(0.12109147, shape=(), dtype=float32)\n",
      "tf.Tensor(0.12083012, shape=(), dtype=float32)\n",
      "tf.Tensor(0.12057514, shape=(), dtype=float32)\n",
      "tf.Tensor(0.12032632, shape=(), dtype=float32)\n",
      "tf.Tensor(0.12008348, shape=(), dtype=float32)\n",
      "tf.Tensor(0.119846396, shape=(), dtype=float32)\n",
      "tf.Tensor(0.1196149, shape=(), dtype=float32)\n",
      "tf.Tensor(0.11938879, shape=(), dtype=float32)\n",
      "tf.Tensor(0.11916791, shape=(), dtype=float32)\n",
      "tf.Tensor(0.118952125, shape=(), dtype=float32)\n",
      "tf.Tensor(0.118741214, shape=(), dtype=float32)\n",
      "tf.Tensor(0.118535064, shape=(), dtype=float32)\n",
      "tf.Tensor(0.11833353, shape=(), dtype=float32)\n",
      "tf.Tensor(0.11813647, shape=(), dtype=float32)\n",
      "tf.Tensor(0.11794375, shape=(), dtype=float32)\n",
      "tf.Tensor(0.11775521, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "opt = SGD(learning_rate=0.1, momentum=0.)\n",
    "n_steps = 100\n",
    "lr = 0.1\n",
    "l2_reg = 0.01\n",
    "betas = tf.Variable(tf.random.normal([31], 0, 0.1), dtype=tf.float32)\n",
    "\n",
    "for ii in range(n_steps):\n",
    "    with tf.GradientTape() as tape:\n",
    "        preds = logistic_function(X_train, betas)\n",
    "        loss = cross_entropy(y_train, preds) + l2_reg*tf.reduce_sum(betas**2)\n",
    "    gradients = tape.gradient(loss, betas)\n",
    "    opt.apply_gradients(zip([gradients], [betas]))\n",
    "    #betas.assign(betas - lr*gradients)\n",
    "    print(loss)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sklearn's logistic regression model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0).fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(), dtype=float32, numpy=0.10721653>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy(y_test, tf.constant(clf.predict_proba(X_test)[:, 1], dtype=tf.float32))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(), dtype=float32, numpy=0.14806245>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy(y_test, logistic_function(X_test, betas))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Custom Stochastic Gradient Descent"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "batch_size = 600"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "n_batches = X_train.shape[0] // batch_size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_batches"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 0.6359825730323792\n",
      "Epoch 1 loss: 0.47558698058128357\n",
      "Epoch 2 loss: 0.3921740651130676\n",
      "Epoch 3 loss: 0.3417300283908844\n",
      "Epoch 4 loss: 0.30728843808174133\n",
      "Epoch 5 loss: 0.28185367584228516\n",
      "Epoch 6 loss: 0.26206982135772705\n",
      "Epoch 7 loss: 0.24611014127731323\n",
      "Epoch 8 loss: 0.2328839749097824\n",
      "Epoch 9 loss: 0.22169384360313416\n",
      "Epoch 10 loss: 0.21206986904144287\n",
      "Epoch 11 loss: 0.20368221402168274\n",
      "Epoch 12 loss: 0.19629140198230743\n",
      "Epoch 13 loss: 0.18971864879131317\n",
      "Epoch 14 loss: 0.18382735550403595\n",
      "Epoch 15 loss: 0.17851091921329498\n",
      "Epoch 16 loss: 0.17368482053279877\n",
      "Epoch 17 loss: 0.16928093135356903\n",
      "Epoch 18 loss: 0.16524362564086914\n",
      "Epoch 19 loss: 0.16152697801589966\n",
      "Epoch 20 loss: 0.15809263288974762\n",
      "Epoch 21 loss: 0.15490826964378357\n",
      "Epoch 22 loss: 0.15194641053676605\n",
      "Epoch 23 loss: 0.14918363094329834\n",
      "Epoch 24 loss: 0.14659962058067322\n",
      "Epoch 25 loss: 0.1441768854856491\n",
      "Epoch 26 loss: 0.14190015196800232\n",
      "Epoch 27 loss: 0.1397559642791748\n",
      "Epoch 28 loss: 0.1377326250076294\n",
      "Epoch 29 loss: 0.1358197182416916\n",
      "Epoch 30 loss: 0.13400796055793762\n",
      "Epoch 31 loss: 0.13228923082351685\n",
      "Epoch 32 loss: 0.13065612316131592\n",
      "Epoch 33 loss: 0.12910209596157074\n",
      "Epoch 34 loss: 0.12762127816677094\n",
      "Epoch 35 loss: 0.12620829045772552\n",
      "Epoch 36 loss: 0.12485835701227188\n",
      "Epoch 37 loss: 0.1235671192407608\n",
      "Epoch 38 loss: 0.12233062833547592\n",
      "Epoch 39 loss: 0.12114526331424713\n",
      "Epoch 40 loss: 0.12000778317451477\n",
      "Epoch 41 loss: 0.1189151406288147\n",
      "Epoch 42 loss: 0.11786459386348724\n",
      "Epoch 43 loss: 0.11685363948345184\n",
      "Epoch 44 loss: 0.11587996780872345\n",
      "Epoch 45 loss: 0.11494136601686478\n",
      "Epoch 46 loss: 0.1140359491109848\n",
      "Epoch 47 loss: 0.11316186189651489\n",
      "Epoch 48 loss: 0.11231740564107895\n",
      "Epoch 49 loss: 0.11150099337100983\n",
      "Epoch 50 loss: 0.1107112392783165\n",
      "Epoch 51 loss: 0.10994673520326614\n",
      "Epoch 52 loss: 0.10920623689889908\n",
      "Epoch 53 loss: 0.10848858952522278\n",
      "Epoch 54 loss: 0.10779264569282532\n",
      "Epoch 55 loss: 0.1071174293756485\n",
      "Epoch 56 loss: 0.10646197199821472\n",
      "Epoch 57 loss: 0.10582536458969116\n",
      "Epoch 58 loss: 0.10520678013563156\n",
      "Epoch 59 loss: 0.10460539907217026\n",
      "Epoch 60 loss: 0.10402048379182816\n",
      "Epoch 61 loss: 0.10345137864351273\n",
      "Epoch 62 loss: 0.10289736092090607\n",
      "Epoch 63 loss: 0.10235784947872162\n",
      "Epoch 64 loss: 0.10183224827051163\n",
      "Epoch 65 loss: 0.10131998360157013\n",
      "Epoch 66 loss: 0.10082055628299713\n",
      "Epoch 67 loss: 0.10033344477415085\n",
      "Epoch 68 loss: 0.09985821694135666\n",
      "Epoch 69 loss: 0.09939438104629517\n",
      "Epoch 70 loss: 0.09894154965877533\n",
      "Epoch 71 loss: 0.09849930554628372\n",
      "Epoch 72 loss: 0.0980672836303711\n",
      "Epoch 73 loss: 0.097645103931427\n",
      "Epoch 74 loss: 0.09723243117332458\n",
      "Epoch 75 loss: 0.09682893007993698\n",
      "Epoch 76 loss: 0.09643427282571793\n",
      "Epoch 77 loss: 0.09604822099208832\n",
      "Epoch 78 loss: 0.09567040950059891\n",
      "Epoch 79 loss: 0.09530063718557358\n",
      "Epoch 80 loss: 0.09493859857320786\n",
      "Epoch 81 loss: 0.09458406269550323\n",
      "Epoch 82 loss: 0.09423680603504181\n",
      "Epoch 83 loss: 0.09389656782150269\n",
      "Epoch 84 loss: 0.09356316924095154\n",
      "Epoch 85 loss: 0.09323637932538986\n",
      "Epoch 86 loss: 0.09291600435972214\n",
      "Epoch 87 loss: 0.09260183572769165\n",
      "Epoch 88 loss: 0.09229373931884766\n",
      "Epoch 89 loss: 0.09199150651693344\n",
      "Epoch 90 loss: 0.09169495850801468\n",
      "Epoch 91 loss: 0.09140396118164062\n",
      "Epoch 92 loss: 0.09111833572387695\n",
      "Epoch 93 loss: 0.09083791822195053\n",
      "Epoch 94 loss: 0.0905626192688942\n",
      "Epoch 95 loss: 0.09029226750135422\n",
      "Epoch 96 loss: 0.09002672135829926\n",
      "Epoch 97 loss: 0.08976586908102036\n",
      "Epoch 98 loss: 0.08950955420732498\n",
      "Epoch 99 loss: 0.08925770968198776\n"
     ]
    }
   ],
   "source": [
    "momentum_rate = 0.1\n",
    "betas_sgd = tf.Variable(tf.random.normal([31], 0, 0.1))\n",
    "update = 0.\n",
    "for epoch in range(n_steps):\n",
    "    random_idx = np.random.permutation(X_train.shape[0])\n",
    "    X_train, y_train = tf.gather(X_train, random_idx), tf.gather(y_train, random_idx)\n",
    "    for ii in range(n_batches+1):\n",
    "        X_batch = X_train[ii*batch_size : (ii+1)*batch_size, :]\n",
    "        y_batch = y_train[ii*batch_size : (ii+1)*batch_size]\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = logistic_function(X_batch, betas_sgd)\n",
    "            batch_loss = cross_entropy(y_batch, y_pred) + l2_reg*tf.reduce_sum(betas_sgd**2)\n",
    "        batch_gradients = tape.gradient(batch_loss, betas_sgd)\n",
    "        update = momentum_rate*update + lr*batch_gradients\n",
    "        betas_sgd.assign(betas_sgd - update)\n",
    "    print(f'Epoch {epoch} loss: {cross_entropy(y_train, logistic_function(X_train, betas_sgd))}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## AdaGrad"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 0.34642294049263\n",
      "Epoch 1 loss: 0.26040250062942505\n",
      "Epoch 2 loss: 0.22292675077915192\n",
      "Epoch 3 loss: 0.19932802021503448\n",
      "Epoch 4 loss: 0.18254263699054718\n",
      "Epoch 5 loss: 0.1698254942893982\n",
      "Epoch 6 loss: 0.15978136658668518\n",
      "Epoch 7 loss: 0.15160518884658813\n",
      "Epoch 8 loss: 0.14479389786720276\n",
      "Epoch 9 loss: 0.1390148252248764\n",
      "Epoch 10 loss: 0.13403798639774323\n",
      "Epoch 11 loss: 0.1296987682580948\n",
      "Epoch 12 loss: 0.12587586045265198\n",
      "Epoch 13 loss: 0.12247776240110397\n",
      "Epoch 14 loss: 0.1194339320063591\n",
      "Epoch 15 loss: 0.11668907850980759\n",
      "Epoch 16 loss: 0.11419917643070221\n",
      "Epoch 17 loss: 0.11192869395017624\n",
      "Epoch 18 loss: 0.10984855890274048\n",
      "Epoch 19 loss: 0.1079348474740982\n",
      "Epoch 20 loss: 0.10616753995418549\n",
      "Epoch 21 loss: 0.10452983528375626\n",
      "Epoch 22 loss: 0.10300745815038681\n",
      "Epoch 23 loss: 0.10158824920654297\n",
      "Epoch 24 loss: 0.10026172548532486\n",
      "Epoch 25 loss: 0.09901885688304901\n",
      "Epoch 26 loss: 0.09785174578428268\n",
      "Epoch 27 loss: 0.09675353020429611\n",
      "Epoch 28 loss: 0.0957181304693222\n",
      "Epoch 29 loss: 0.09474024176597595\n",
      "Epoch 30 loss: 0.09381510317325592\n",
      "Epoch 31 loss: 0.09293852746486664\n",
      "Epoch 32 loss: 0.09210672974586487\n",
      "Epoch 33 loss: 0.0913163423538208\n",
      "Epoch 34 loss: 0.09056436270475388\n",
      "Epoch 35 loss: 0.08984803408384323\n",
      "Epoch 36 loss: 0.08916487544775009\n",
      "Epoch 37 loss: 0.08851268142461777\n",
      "Epoch 38 loss: 0.08788937330245972\n",
      "Epoch 39 loss: 0.0872931033372879\n",
      "Epoch 40 loss: 0.08672218024730682\n",
      "Epoch 41 loss: 0.08617504686117172\n",
      "Epoch 42 loss: 0.08565028011798859\n",
      "Epoch 43 loss: 0.08514655381441116\n",
      "Epoch 44 loss: 0.08466267585754395\n",
      "Epoch 45 loss: 0.0841975137591362\n",
      "Epoch 46 loss: 0.08375004678964615\n",
      "Epoch 47 loss: 0.08331931382417679\n",
      "Epoch 48 loss: 0.08290442824363708\n",
      "Epoch 49 loss: 0.08250457048416138\n",
      "Epoch 50 loss: 0.08211896568536758\n",
      "Epoch 51 loss: 0.081746906042099\n",
      "Epoch 52 loss: 0.0813877210021019\n",
      "Epoch 53 loss: 0.08104078471660614\n",
      "Epoch 54 loss: 0.08070552349090576\n",
      "Epoch 55 loss: 0.08038138598203659\n",
      "Epoch 56 loss: 0.08006785809993744\n",
      "Epoch 57 loss: 0.0797644779086113\n",
      "Epoch 58 loss: 0.07947074621915817\n",
      "Epoch 59 loss: 0.07918629050254822\n",
      "Epoch 60 loss: 0.07891068607568741\n",
      "Epoch 61 loss: 0.0786435529589653\n",
      "Epoch 62 loss: 0.07838454842567444\n",
      "Epoch 63 loss: 0.07813332229852676\n",
      "Epoch 64 loss: 0.0778895691037178\n",
      "Epoch 65 loss: 0.07765297591686249\n",
      "Epoch 66 loss: 0.07742326706647873\n",
      "Epoch 67 loss: 0.07720017433166504\n",
      "Epoch 68 loss: 0.07698342949151993\n",
      "Epoch 69 loss: 0.0767727941274643\n",
      "Epoch 70 loss: 0.07656805217266083\n",
      "Epoch 71 loss: 0.07636896520853043\n",
      "Epoch 72 loss: 0.07617532461881638\n",
      "Epoch 73 loss: 0.07598695158958435\n",
      "Epoch 74 loss: 0.07580363750457764\n",
      "Epoch 75 loss: 0.0756252110004425\n",
      "Epoch 76 loss: 0.07545150071382523\n",
      "Epoch 77 loss: 0.07528232038021088\n",
      "Epoch 78 loss: 0.0751175507903099\n",
      "Epoch 79 loss: 0.07495701313018799\n",
      "Epoch 80 loss: 0.07480058819055557\n",
      "Epoch 81 loss: 0.07464811205863953\n",
      "Epoch 82 loss: 0.0744994729757309\n",
      "Epoch 83 loss: 0.07435454428195953\n",
      "Epoch 84 loss: 0.07421320676803589\n",
      "Epoch 85 loss: 0.07407531887292862\n",
      "Epoch 86 loss: 0.07394080609083176\n",
      "Epoch 87 loss: 0.07380954176187515\n",
      "Epoch 88 loss: 0.07368142902851105\n",
      "Epoch 89 loss: 0.07355638593435287\n",
      "Epoch 90 loss: 0.07343429327011108\n",
      "Epoch 91 loss: 0.0733150765299797\n",
      "Epoch 92 loss: 0.07319863885641098\n",
      "Epoch 93 loss: 0.07308491319417953\n",
      "Epoch 94 loss: 0.07297380268573761\n",
      "Epoch 95 loss: 0.07286524027585983\n",
      "Epoch 96 loss: 0.07275913655757904\n",
      "Epoch 97 loss: 0.07265544682741165\n",
      "Epoch 98 loss: 0.0725540816783905\n",
      "Epoch 99 loss: 0.07245498150587082\n"
     ]
    }
   ],
   "source": [
    "momentum_rate = 0.1\n",
    "betas_adagrad = tf.Variable(tf.random.normal([31], 0, 0.1))\n",
    "update = 0.\n",
    "G_diag = tf.Variable(tf.zeros(betas_adagrad.shape), trainable=False)\n",
    "for epoch in range(n_steps):\n",
    "    random_idx = np.random.permutation(X_train.shape[0])\n",
    "    X_train, y_train = tf.gather(X_train, random_idx), tf.gather(y_train, random_idx)\n",
    "    for ii in range(n_batches+1):\n",
    "        X_batch = X_train[ii*batch_size : (ii+1)*batch_size, :]\n",
    "        y_batch = y_train[ii*batch_size : (ii+1)*batch_size]\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = logistic_function(X_batch, betas_adagrad)\n",
    "            batch_loss = cross_entropy(y_batch, y_pred) + l2_reg*tf.reduce_sum(betas_adagrad**2)\n",
    "        batch_gradients = tape.gradient(batch_loss, betas_adagrad)\n",
    "        G_diag.assign(G_diag + batch_gradients**2)\n",
    "        update = lr*batch_gradients / tf.math.sqrt(G_diag)\n",
    "        betas_adagrad.assign(betas_adagrad - update)\n",
    "    print(f'Epoch {epoch} loss: {cross_entropy(y_train, logistic_function(X_train, betas_adagrad))}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RMS Prop"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 0.2946400046348572\n",
      "Epoch 1 loss: 0.15876005589962006\n",
      "Epoch 2 loss: 0.10068787634372711\n",
      "Epoch 3 loss: 0.08193618059158325\n",
      "Epoch 4 loss: 0.07754947245121002\n",
      "Epoch 5 loss: 0.0792754590511322\n",
      "Epoch 6 loss: 0.08013128489255905\n",
      "Epoch 7 loss: 0.07874292880296707\n",
      "Epoch 8 loss: 0.07913576066493988\n",
      "Epoch 9 loss: 0.07830916345119476\n",
      "Epoch 10 loss: 0.07836197316646576\n",
      "Epoch 11 loss: 0.07794619351625443\n",
      "Epoch 12 loss: 0.07772601395845413\n",
      "Epoch 13 loss: 0.07762741297483444\n",
      "Epoch 14 loss: 0.07718892395496368\n",
      "Epoch 15 loss: 0.07734014093875885\n",
      "Epoch 16 loss: 0.07672737538814545\n",
      "Epoch 17 loss: 0.07707707583904266\n",
      "Epoch 18 loss: 0.07632561028003693\n",
      "Epoch 19 loss: 0.07683362066745758\n",
      "Epoch 20 loss: 0.07597237825393677\n",
      "Epoch 21 loss: 0.0766066461801529\n",
      "Epoch 22 loss: 0.07565917819738388\n",
      "Epoch 23 loss: 0.07639393955469131\n",
      "Epoch 24 loss: 0.07537949830293655\n",
      "Epoch 25 loss: 0.07619379460811615\n",
      "Epoch 26 loss: 0.0751282274723053\n",
      "Epoch 27 loss: 0.07600495964288712\n",
      "Epoch 28 loss: 0.07490122318267822\n",
      "Epoch 29 loss: 0.07582634687423706\n",
      "Epoch 30 loss: 0.07469511777162552\n",
      "Epoch 31 loss: 0.07565711438655853\n",
      "Epoch 32 loss: 0.07450716197490692\n",
      "Epoch 33 loss: 0.07549651712179184\n",
      "Epoch 34 loss: 0.07433507591485977\n",
      "Epoch 35 loss: 0.07534389942884445\n",
      "Epoch 36 loss: 0.07417690008878708\n",
      "Epoch 37 loss: 0.075198695063591\n",
      "Epoch 38 loss: 0.0740310400724411\n",
      "Epoch 39 loss: 0.0750604122877121\n",
      "Epoch 40 loss: 0.07389608025550842\n",
      "Epoch 41 loss: 0.07492858171463013\n",
      "Epoch 42 loss: 0.07377085834741592\n",
      "Epoch 43 loss: 0.07480277866125107\n",
      "Epoch 44 loss: 0.07365436106920242\n",
      "Epoch 45 loss: 0.07468264549970627\n",
      "Epoch 46 loss: 0.07354568690061569\n",
      "Epoch 47 loss: 0.07456781715154648\n",
      "Epoch 48 loss: 0.07344406843185425\n",
      "Epoch 49 loss: 0.07445798069238663\n",
      "Epoch 50 loss: 0.07334885001182556\n",
      "Epoch 51 loss: 0.07435283064842224\n",
      "Epoch 52 loss: 0.07325944304466248\n",
      "Epoch 53 loss: 0.07425212115049362\n",
      "Epoch 54 loss: 0.0731753408908844\n",
      "Epoch 55 loss: 0.07415558397769928\n",
      "Epoch 56 loss: 0.07309604436159134\n",
      "Epoch 57 loss: 0.07406298816204071\n",
      "Epoch 58 loss: 0.07302118837833405\n",
      "Epoch 59 loss: 0.0739741101861\n",
      "Epoch 60 loss: 0.07295040041208267\n",
      "Epoch 61 loss: 0.07388877123594284\n",
      "Epoch 62 loss: 0.07288335263729095\n",
      "Epoch 63 loss: 0.0738067552447319\n",
      "Epoch 64 loss: 0.0728197693824768\n",
      "Epoch 65 loss: 0.07372791320085526\n",
      "Epoch 66 loss: 0.07275937497615814\n",
      "Epoch 67 loss: 0.07365206629037857\n",
      "Epoch 68 loss: 0.07270194590091705\n",
      "Epoch 69 loss: 0.07357907295227051\n",
      "Epoch 70 loss: 0.07264726608991623\n",
      "Epoch 71 loss: 0.07350879162549973\n",
      "Epoch 72 loss: 0.07259514182806015\n",
      "Epoch 73 loss: 0.07344108074903488\n",
      "Epoch 74 loss: 0.07254540175199509\n",
      "Epoch 75 loss: 0.07337581366300583\n",
      "Epoch 76 loss: 0.0724978968501091\n",
      "Epoch 77 loss: 0.0733129009604454\n",
      "Epoch 78 loss: 0.07245247066020966\n",
      "Epoch 79 loss: 0.07325219362974167\n",
      "Epoch 80 loss: 0.0724090114235878\n",
      "Epoch 81 loss: 0.07319361716508865\n",
      "Epoch 82 loss: 0.072367362678051\n",
      "Epoch 83 loss: 0.073137067258358\n",
      "Epoch 84 loss: 0.07232743501663208\n",
      "Epoch 85 loss: 0.07308243960142136\n",
      "Epoch 86 loss: 0.0722891241312027\n",
      "Epoch 87 loss: 0.07302967458963394\n",
      "Epoch 88 loss: 0.07225234806537628\n",
      "Epoch 89 loss: 0.0729786604642868\n",
      "Epoch 90 loss: 0.07221698760986328\n",
      "Epoch 91 loss: 0.07292934507131577\n",
      "Epoch 92 loss: 0.07218299061059952\n",
      "Epoch 93 loss: 0.07288163900375366\n",
      "Epoch 94 loss: 0.07215028256177902\n",
      "Epoch 95 loss: 0.07283548265695572\n",
      "Epoch 96 loss: 0.07211878150701523\n",
      "Epoch 97 loss: 0.07279081642627716\n",
      "Epoch 98 loss: 0.07208842784166336\n",
      "Epoch 99 loss: 0.07274757325649261\n"
     ]
    }
   ],
   "source": [
    "betas_rms = tf.Variable(tf.random.normal([31], 0, 0.1))\n",
    "forgetting_factor = 0.01\n",
    "running_avg = tf.Variable(tf.zeros(betas_rms.shape), trainable=False)\n",
    "for epoch in range(n_steps):\n",
    "    random_idx = np.random.permutation(X_train.shape[0])\n",
    "    X_train, y_train = tf.gather(X_train, random_idx), tf.gather(y_train, random_idx)\n",
    "    for ii in range(n_batches+1):\n",
    "        X_batch = X_train[ii*batch_size : (ii+1)*batch_size, :]\n",
    "        y_batch = y_train[ii*batch_size : (ii+1)*batch_size]\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = logistic_function(X_batch, betas_rms)\n",
    "            batch_loss = cross_entropy(y_batch, y_pred) + l2_reg*tf.reduce_sum(betas_rms**2)\n",
    "        batch_gradients = tape.gradient(batch_loss, betas_rms)\n",
    "        running_avg.assign(forgetting_factor*running_avg + (1-forgetting_factor)*batch_gradients**2)\n",
    "        update = lr*batch_gradients / tf.math.sqrt(running_avg)\n",
    "        betas_rms.assign(betas_rms - update)\n",
    "    print(f'Epoch {epoch} loss: {cross_entropy(y_train, logistic_function(X_train, betas_rms))}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ADAM"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 0.22991017997264862\n",
      "Epoch 1 loss: 0.15012414753437042\n",
      "Epoch 2 loss: 0.11079052835702896\n",
      "Epoch 3 loss: 0.08721912652254105\n",
      "Epoch 4 loss: 0.07278712093830109\n",
      "Epoch 5 loss: 0.06394709646701813\n",
      "Epoch 6 loss: 0.058598510921001434\n",
      "Epoch 7 loss: 0.055399179458618164\n",
      "Epoch 8 loss: 0.05338878929615021\n",
      "Epoch 9 loss: 0.05190142244100571\n",
      "Epoch 10 loss: 0.05054578185081482\n",
      "Epoch 11 loss: 0.04916718974709511\n",
      "Epoch 12 loss: 0.04778120294213295\n",
      "Epoch 13 loss: 0.046496693044900894\n",
      "Epoch 14 loss: 0.04544699192047119\n",
      "Epoch 15 loss: 0.04473799467086792\n",
      "Epoch 16 loss: 0.044417038559913635\n",
      "Epoch 17 loss: 0.04446619749069214\n",
      "Epoch 18 loss: 0.04481950029730797\n",
      "Epoch 19 loss: 0.0453931987285614\n",
      "Epoch 20 loss: 0.046114273369312286\n",
      "Epoch 21 loss: 0.046938054263591766\n",
      "Epoch 22 loss: 0.0478539802134037\n",
      "Epoch 23 loss: 0.04888114333152771\n",
      "Epoch 24 loss: 0.0500560961663723\n",
      "Epoch 25 loss: 0.05141536518931389\n",
      "Epoch 26 loss: 0.05297780781984329\n",
      "Epoch 27 loss: 0.05473363399505615\n",
      "Epoch 28 loss: 0.056645460426807404\n",
      "Epoch 29 loss: 0.05866071581840515\n",
      "Epoch 30 loss: 0.06072891876101494\n",
      "Epoch 31 loss: 0.06281445920467377\n",
      "Epoch 32 loss: 0.0648985207080841\n",
      "Epoch 33 loss: 0.06696821749210358\n",
      "Epoch 34 loss: 0.06899858266115189\n",
      "Epoch 35 loss: 0.0709381178021431\n",
      "Epoch 36 loss: 0.07270937412977219\n",
      "Epoch 37 loss: 0.07422705739736557\n",
      "Epoch 38 loss: 0.07542341947555542\n",
      "Epoch 39 loss: 0.0762656182050705\n",
      "Epoch 40 loss: 0.07675555348396301\n",
      "Epoch 41 loss: 0.07691673189401627\n",
      "Epoch 42 loss: 0.07677891850471497\n",
      "Epoch 43 loss: 0.07637003809213638\n",
      "Epoch 44 loss: 0.07571735233068466\n",
      "Epoch 45 loss: 0.07485496252775192\n",
      "Epoch 46 loss: 0.07382998615503311\n",
      "Epoch 47 loss: 0.07270166277885437\n",
      "Epoch 48 loss: 0.07153283804655075\n",
      "Epoch 49 loss: 0.07037921249866486\n",
      "Epoch 50 loss: 0.06928224861621857\n",
      "Epoch 51 loss: 0.06826788187026978\n",
      "Epoch 52 loss: 0.06735018640756607\n",
      "Epoch 53 loss: 0.06653635948896408\n",
      "Epoch 54 loss: 0.06583058089017868\n",
      "Epoch 55 loss: 0.06523532420396805\n",
      "Epoch 56 loss: 0.06475062668323517\n",
      "Epoch 57 loss: 0.0643732026219368\n",
      "Epoch 58 loss: 0.06409645080566406\n",
      "Epoch 59 loss: 0.06391200423240662\n",
      "Epoch 60 loss: 0.06381183117628098\n",
      "Epoch 61 loss: 0.06378977000713348\n",
      "Epoch 62 loss: 0.06384161114692688\n",
      "Epoch 63 loss: 0.06396379321813583\n",
      "Epoch 64 loss: 0.06415152549743652\n",
      "Epoch 65 loss: 0.06439757347106934\n",
      "Epoch 66 loss: 0.06469231843948364\n",
      "Epoch 67 loss: 0.06502476334571838\n",
      "Epoch 68 loss: 0.06538356095552444\n",
      "Epoch 69 loss: 0.0657576322555542\n",
      "Epoch 70 loss: 0.06613577902317047\n",
      "Epoch 71 loss: 0.0665062740445137\n",
      "Epoch 72 loss: 0.06685696542263031\n",
      "Epoch 73 loss: 0.06717611104249954\n",
      "Epoch 74 loss: 0.06745398044586182\n",
      "Epoch 75 loss: 0.06768389046192169\n",
      "Epoch 76 loss: 0.06786263734102249\n",
      "Epoch 77 loss: 0.06798989325761795\n",
      "Epoch 78 loss: 0.06806720048189163\n",
      "Epoch 79 loss: 0.06809726357460022\n",
      "Epoch 80 loss: 0.06808368116617203\n",
      "Epoch 81 loss: 0.06803107261657715\n",
      "Epoch 82 loss: 0.06794509291648865\n",
      "Epoch 83 loss: 0.06783214956521988\n",
      "Epoch 84 loss: 0.06769885867834091\n",
      "Epoch 85 loss: 0.0675516128540039\n",
      "Epoch 86 loss: 0.06739631295204163\n",
      "Epoch 87 loss: 0.0672384649515152\n",
      "Epoch 88 loss: 0.06708332151174545\n",
      "Epoch 89 loss: 0.06693590432405472\n",
      "Epoch 90 loss: 0.0668008103966713\n",
      "Epoch 91 loss: 0.06668194383382797\n",
      "Epoch 92 loss: 0.06658218055963516\n",
      "Epoch 93 loss: 0.06650330126285553\n",
      "Epoch 94 loss: 0.06644593179225922\n",
      "Epoch 95 loss: 0.06640966981649399\n",
      "Epoch 96 loss: 0.06639318913221359\n",
      "Epoch 97 loss: 0.06639441102743149\n",
      "Epoch 98 loss: 0.06641069799661636\n",
      "Epoch 99 loss: 0.06643916666507721\n"
     ]
    }
   ],
   "source": [
    "betas_adam = tf.Variable(tf.random.normal([31], 0, 0.1))\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.999\n",
    "epsilon = 1e-7\n",
    "running_avg = tf.Variable(tf.zeros(betas_adam.shape), trainable=False)\n",
    "running_var = tf.Variable(tf.zeros(betas_adam.shape), trainable=False)\n",
    "running_avg_hat = tf.Variable(tf.zeros(betas_adam.shape), trainable=False)\n",
    "running_var_hat = tf.Variable(tf.zeros(betas_adam.shape), trainable=False)\n",
    "training_iter = 1\n",
    "for epoch in range(n_steps):\n",
    "    random_idx = np.random.permutation(X_train.shape[0])\n",
    "    X_train, y_train = tf.gather(X_train, random_idx), tf.gather(y_train, random_idx)\n",
    "    for ii in range(n_batches+1):\n",
    "        X_batch = X_train[ii*batch_size : (ii+1)*batch_size, :]\n",
    "        y_batch = y_train[ii*batch_size : (ii+1)*batch_size]\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = logistic_function(X_batch, betas_adam)\n",
    "            batch_loss = cross_entropy(y_batch, y_pred) + l2_reg*tf.reduce_sum(betas_adam**2)\n",
    "        batch_gradients = tape.gradient(batch_loss, betas_adam)\n",
    "        running_avg.assign(beta_1*running_avg + (1-beta_1)*batch_gradients)\n",
    "        running_var.assign(beta_2*running_var + (1-beta_2)*batch_gradients**2)\n",
    "        running_avg_hat.assign(running_avg/(1-beta_1**training_iter))\n",
    "        running_var_hat.assign(running_var / (1-beta_2**training_iter))\n",
    "        update = lr * running_avg_hat / (tf.math.sqrt(running_var_hat) + epsilon)\n",
    "        betas_adam.assign(betas_adam - update)\n",
    "        training_iter += 1\n",
    "    print(f'Epoch {epoch} loss: {cross_entropy(y_train, logistic_function(X_train, betas_adam))}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class Optimizer(ABC):\n",
    "    def __init__(self, lr = 0.1):\n",
    "        self.lr = lr\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_update(self, gradients):\n",
    "        raise NotImplementedError('This method is not implemented for the parent Optimizer class.')\n",
    "\n",
    "    def apply_gradients(self, gradients, variables):\n",
    "        update = self.get_update(gradients)\n",
    "        variables.assign(variables - update)\n",
    "\n",
    "\n",
    "class ADAM(Optimizer):\n",
    "    def __init__(self, beta_1 = 0.9, beta_2 = 0.999, var_shape = (31), epsilon = 1e-7, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        self.epsilon = epsilon\n",
    "        self.running_avg = tf.Variable(tf.zeros(var_shape), trainable=False)\n",
    "        self.running_var = tf.Variable(tf.zeros(var_shape), trainable=False)\n",
    "        self.training_iter = 1\n",
    "\n",
    "    def get_update(self, gradients):\n",
    "        self.running_avg.assign(self.beta_1*self.running_avg + (1-self.beta_1)*gradients)\n",
    "        self.running_var.assign(self.beta_2*self.running_var + (1-self.beta_2)*gradients**2)\n",
    "        running_avg_hat = self.running_avg / (1-self.beta_1**self.training_iter)\n",
    "        running_var_hat = self.running_var / (1-self.beta_2**self.training_iter)\n",
    "        self.training_iter += 1\n",
    "        return self.lr * running_avg_hat / (tf.math.sqrt(running_var_hat) + self.epsilon)\n",
    "\n",
    "\n",
    "class RMSProp(Optimizer):\n",
    "    def __init__(self, forgetting_factor = 0.01, var_shape = (31), **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.forgetting_factor = forgetting_factor\n",
    "        self.running_avg = tf.Variable(tf.zeros(var_shape), trainable=False)\n",
    "\n",
    "    def get_update(self, gradients):\n",
    "        self.running_avg.assign(self.forgetting_factor*self.running_avg + (1-self.forgetting_factor)*gradients**2)\n",
    "        return self.lr*gradients / tf.math.sqrt(self.running_avg)\n",
    "\n",
    "\n",
    "class AdaGrad(Optimizer):\n",
    "    def __init__(self, var_shape = (31), **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.G_diag = tf.Variable(tf.zeros(var_shape), trainable=False)\n",
    "\n",
    "    def get_update(self, gradients):\n",
    "        self.G_diag.assign(self.G_diag + gradients**2)\n",
    "        return self.lr*gradients / tf.math.sqrt(self.G_diag)\n",
    "\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, momentum_rate = 0.,  **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.update = 0.\n",
    "        self.momentum_rate = momentum_rate\n",
    "\n",
    "    def get_update(self, gradients):\n",
    "        self.update = self.momentum_rate*self.update + self.lr*gradients\n",
    "        return self.update"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 0.5073832869529724\n",
      "Epoch 1 loss: 0.4158197045326233\n",
      "Epoch 2 loss: 0.3604734241962433\n",
      "Epoch 3 loss: 0.3231218159198761\n",
      "Epoch 4 loss: 0.2957425117492676\n",
      "Epoch 5 loss: 0.27453312277793884\n",
      "Epoch 6 loss: 0.2574610710144043\n",
      "Epoch 7 loss: 0.24332869052886963\n",
      "Epoch 8 loss: 0.23137733340263367\n",
      "Epoch 9 loss: 0.22109933197498322\n",
      "Epoch 10 loss: 0.21213982999324799\n",
      "Epoch 11 loss: 0.20424221456050873\n",
      "Epoch 12 loss: 0.1972152441740036\n",
      "Epoch 13 loss: 0.19091317057609558\n",
      "Epoch 14 loss: 0.18522241711616516\n",
      "Epoch 15 loss: 0.1800529956817627\n",
      "Epoch 16 loss: 0.1753324717283249\n",
      "Epoch 17 loss: 0.17100180685520172\n",
      "Epoch 18 loss: 0.16701219975948334\n",
      "Epoch 19 loss: 0.16332300007343292\n",
      "Epoch 20 loss: 0.15989995002746582\n",
      "Epoch 21 loss: 0.15671396255493164\n",
      "Epoch 22 loss: 0.15374021232128143\n",
      "Epoch 23 loss: 0.15095725655555725\n",
      "Epoch 24 loss: 0.14834654331207275\n",
      "Epoch 25 loss: 0.14589188992977142\n",
      "Epoch 26 loss: 0.1435791552066803\n",
      "Epoch 27 loss: 0.14139588177204132\n",
      "Epoch 28 loss: 0.13933102786540985\n",
      "Epoch 29 loss: 0.1373748481273651\n",
      "Epoch 30 loss: 0.1355186402797699\n",
      "Epoch 31 loss: 0.13375461101531982\n",
      "Epoch 32 loss: 0.13207578659057617\n",
      "Epoch 33 loss: 0.13047586381435394\n",
      "Epoch 34 loss: 0.12894923985004425\n",
      "Epoch 35 loss: 0.12749072909355164\n",
      "Epoch 36 loss: 0.1260957270860672\n",
      "Epoch 37 loss: 0.12476000934839249\n",
      "Epoch 38 loss: 0.12347970902919769\n",
      "Epoch 39 loss: 0.12225130200386047\n",
      "Epoch 40 loss: 0.12107161432504654\n",
      "Epoch 41 loss: 0.11993765085935593\n",
      "Epoch 42 loss: 0.11884672194719315\n",
      "Epoch 43 loss: 0.11779630184173584\n",
      "Epoch 44 loss: 0.11678412556648254\n",
      "Epoch 45 loss: 0.11580805480480194\n",
      "Epoch 46 loss: 0.11486612260341644\n",
      "Epoch 47 loss: 0.11395647376775742\n",
      "Epoch 48 loss: 0.11307744681835175\n",
      "Epoch 49 loss: 0.11222745478153229\n",
      "Epoch 50 loss: 0.11140502989292145\n",
      "Epoch 51 loss: 0.11060881614685059\n",
      "Epoch 52 loss: 0.10983754694461823\n",
      "Epoch 53 loss: 0.10909000784158707\n",
      "Epoch 54 loss: 0.10836508125066757\n",
      "Epoch 55 loss: 0.10766176134347916\n",
      "Epoch 56 loss: 0.10697904229164124\n",
      "Epoch 57 loss: 0.1063159927725792\n",
      "Epoch 58 loss: 0.10567176342010498\n",
      "Epoch 59 loss: 0.10504554957151413\n",
      "Epoch 60 loss: 0.10443656146526337\n",
      "Epoch 61 loss: 0.10384411364793777\n",
      "Epoch 62 loss: 0.10326749831438065\n",
      "Epoch 63 loss: 0.10270606726408005\n",
      "Epoch 64 loss: 0.10215923190116882\n",
      "Epoch 65 loss: 0.10162640362977982\n",
      "Epoch 66 loss: 0.10110704600811005\n",
      "Epoch 67 loss: 0.10060065984725952\n",
      "Epoch 68 loss: 0.10010670870542526\n",
      "Epoch 69 loss: 0.09962478280067444\n",
      "Epoch 70 loss: 0.09915440529584885\n",
      "Epoch 71 loss: 0.09869518131017685\n",
      "Epoch 72 loss: 0.09824668616056442\n",
      "Epoch 73 loss: 0.0978085845708847\n",
      "Epoch 74 loss: 0.09738045185804367\n",
      "Epoch 75 loss: 0.09696200489997864\n",
      "Epoch 76 loss: 0.09655285626649857\n",
      "Epoch 77 loss: 0.09615276008844376\n",
      "Epoch 78 loss: 0.09576138108968735\n",
      "Epoch 79 loss: 0.09537842869758606\n",
      "Epoch 80 loss: 0.095003642141819\n",
      "Epoch 81 loss: 0.09463676810264587\n",
      "Epoch 82 loss: 0.09427753835916519\n",
      "Epoch 83 loss: 0.09392572194337845\n",
      "Epoch 84 loss: 0.09358109533786774\n",
      "Epoch 85 loss: 0.09324343502521515\n",
      "Epoch 86 loss: 0.09291251748800278\n",
      "Epoch 87 loss: 0.09258817881345749\n",
      "Epoch 88 loss: 0.09227019548416138\n",
      "Epoch 89 loss: 0.09195837378501892\n",
      "Epoch 90 loss: 0.09165256470441818\n",
      "Epoch 91 loss: 0.09135258197784424\n",
      "Epoch 92 loss: 0.09105826914310455\n",
      "Epoch 93 loss: 0.0907694473862648\n",
      "Epoch 94 loss: 0.09048598259687424\n",
      "Epoch 95 loss: 0.09020771086215973\n",
      "Epoch 96 loss: 0.08993451297283173\n",
      "Epoch 97 loss: 0.08966626226902008\n",
      "Epoch 98 loss: 0.08940277248620987\n",
      "Epoch 99 loss: 0.08914396911859512\n"
     ]
    }
   ],
   "source": [
    "betas_adam = tf.Variable(tf.random.normal([31], 0, 0.1))\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.999\n",
    "epsilon = 1e-7\n",
    "\n",
    "adam = ADAM(lr=0.01, beta_1=beta_1, beta_2 = beta_2)\n",
    "rmsprop = RMSProp(lr = lr, forgetting_factor = 0.01)\n",
    "adagrad = AdaGrad(lr = lr)\n",
    "sgd = SGD(lr = lr, momentum_rate= 0.1)\n",
    "for epoch in range(n_steps):\n",
    "    random_idx = np.random.permutation(X_train.shape[0])\n",
    "    X_train, y_train = tf.gather(X_train, random_idx), tf.gather(y_train, random_idx)\n",
    "    for ii in range(n_batches+1):\n",
    "        X_batch = X_train[ii*batch_size : (ii+1)*batch_size, :]\n",
    "        y_batch = y_train[ii*batch_size : (ii+1)*batch_size]\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = logistic_function(X_batch, betas_adam)\n",
    "            batch_loss = cross_entropy(y_batch, y_pred) + l2_reg*tf.reduce_sum(betas_adam**2)\n",
    "        batch_gradients = tape.gradient(batch_loss, betas_adam)\n",
    "        sgd.apply_gradients(batch_gradients, betas_adam)\n",
    "    print(f'Epoch {epoch} loss: {cross_entropy(y_train, logistic_function(X_train, betas_adam))}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# todo:  zrobic jakas ladna petle z eksperymentami (rozne parametry + porownanie z kerasowymi optymizerami), zwizualizowac loss function (albo jako przebieg, albo jak taka siatke wartosci koncowych/srednich jak zrobil Krubeal, porownac najlepszy optimizer (lub kilka) z LogisticRegression.fit implementacji sklearn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}